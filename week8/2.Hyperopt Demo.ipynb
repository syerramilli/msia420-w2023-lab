{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acbe050f",
   "metadata": {},
   "source": [
    "# Tree-structured Parzen estimators\n",
    "\n",
    "In the lectures and the lab so far, we have used grid search to tune the hyperparameters of models. However, it is not a scalable strategy - the number of hyperparameter combinations increases exponentially with the number of hyperparameters.\n",
    "\n",
    "In the last decade or so, there has been a lot of research in developing better search/optimization algorithms for hyperparameter optimization that require much less exploration of the hyperparameter space than grid search. We will look at one such method: the [Tree-structured Parzen Estimator (TPE)](https://papers.nips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html) algorithm. While its often not the best performing hyperparameter optimization algorithm among recent advances across different settings, it is much more widely applicable, is much more easier to implement than the other algorithms, and can be easily scaled. The `hyperopt` library in Python contains an implementation of the TPE algorithm \n",
    "\n",
    "In this notebook, we will consider a simple example of tuning a single layer `MLPRegression` model on the concrete dataset. In the accompanying R script in the lab, we will use it to tune the hyperparameters of the `nnet` function. In the next lab, we use it for tuning gradient boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eea5ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ab41b",
   "metadata": {},
   "source": [
    "## Read, load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ecf7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the concrete dataset\n",
    "crt = pd.read_csv(\"../data/concrete.csv\")\n",
    "\n",
    "# standardize predictors\n",
    "X = crt.drop('Strength',axis=1).values # extract as numpy ndarray\n",
    "X_mean,X_std = X.mean(axis=0),X.std(axis=0)\n",
    "X = (X-X_mean)/X_std\n",
    "\n",
    "# standardize response\n",
    "y = crt['Strength'].values\n",
    "y_mean,y_std = y.mean(),y.std()\n",
    "y = (y-y_mean)/y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19741a37",
   "metadata": {},
   "source": [
    "## The TPE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2866440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt imports\n",
    "from hyperopt import hp, STATUS_OK,fmin,tpe, Trials, space_eval\n",
    "\n",
    "# take care of convergence warnings\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5258d74",
   "metadata": {},
   "source": [
    "The first step is to define the search space. In the case of neural network regression model (with deterministic optimization), there are 3 hyperparameters.\n",
    "\n",
    "\n",
    "1. `hidden_layer_sizes`: the number of units in the single hidden layer. This is an integer hyperparameter.\n",
    "2. `alpha`: the L2 regularization hyperparameter. This is a continuous hyperarameter which we will optimize in the log-scale. \n",
    "3. `activation`: the choice of the activation function. One of `\"relu\"`, `\"tanh\"`, `\"logistic\"`. We chose a logistic activation function in Weeks 3 and 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab47b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "     # integer-valued hyperparameter that can take on values 2...40.\n",
    "    'hidden_layer_sizes':hp.quniform('hidden_layer_sizes',2,40,q=1),\n",
    "    # hyperparameter optimized in log-scale\n",
    "    'alpha':hp.loguniform('alpha',np.log(1e-4),np.log(1)),\n",
    "    # categorical hyperparameter,\n",
    "    'activation':hp.choice('activation',['relu','tanh','logistic'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bc625",
   "metadata": {},
   "source": [
    "The second step is to define a **loss** function. Here, we will use the 5-fold cross-validation root mean squared error (RMSE) as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b4be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(5,random_state=1,shuffle=True)\n",
    "train_test_splits = list(cv.split(X))\n",
    "\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def cv_mse(config):\n",
    "    K = cv.get_n_splits()\n",
    "    rmse_folds = [None]*K\n",
    "    \n",
    "    for k,(train_index,test_index) in enumerate(train_test_splits):\n",
    "        \n",
    "        # fit a neural network model with the given configuration on the training data\n",
    "        nn_fold = MLPRegressor(\n",
    "            hidden_layer_sizes=int(config['hidden_layer_sizes']),\n",
    "            activation = config['activation'],\n",
    "            alpha = config['alpha'],\n",
    "            # optimization settings\n",
    "            solver='lbfgs',max_iter=1000\n",
    "        ).fit(X[train_index,:],y[train_index])\n",
    "        \n",
    "        # obtain predictions on the holdout fold\n",
    "        y_pred = nn_fold.predict(X[test_index,:])\n",
    "        \n",
    "        # compute RMSE\n",
    "        rmse_folds[k] = np.sqrt(mean_squared_error(y[test_index],y_pred))\n",
    "        \n",
    "    \n",
    "    return {'loss':np.mean(rmse_folds),'status':STATUS_OK} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53aa83c",
   "metadata": {},
   "source": [
    "The third step is to use the `fmin` function to run the hyperopt algorithm. \n",
    "\n",
    "**Note**: You can get somewhat different results because `MLPRegressor` fits are noisy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "207c30f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:17<00:00,  2.58s/trial, best loss: 0.287045197187734]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.016044828504314324,\n",
       " 'hidden_layer_sizes': 26.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials() # maintains an optimization history\n",
    "best = fmin(\n",
    "    cv_mse, # loss function\n",
    "    search_space, # search space\n",
    "    algo=tpe.suggest,\n",
    "    max_evals = 30, # number of hyperparameter configurations to explore\n",
    "    trials=trials,\n",
    "    return_argmin=False,\n",
    "    rstate=np.random.default_rng(1)\n",
    ")\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2145801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>alpha</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>RMSE_CV</th>\n",
       "      <th>R2_cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.016045</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.287045</td>\n",
       "      <td>0.917605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.287388</td>\n",
       "      <td>0.917408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.015591</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.290107</td>\n",
       "      <td>0.915838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.104334</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.301130</td>\n",
       "      <td>0.909321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.303154</td>\n",
       "      <td>0.908097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.020123</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.307135</td>\n",
       "      <td>0.905668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>0.160314</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.308621</td>\n",
       "      <td>0.904753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.025826</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.308811</td>\n",
       "      <td>0.904636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.005094</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.309557</td>\n",
       "      <td>0.904175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tanh</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.310537</td>\n",
       "      <td>0.903567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation     alpha  hidden_layer_sizes   RMSE_CV     R2_cv\n",
       "13       relu  0.016045                26.0  0.287045  0.917605\n",
       "10       relu  0.009092                29.0  0.287388  0.917408\n",
       "9        relu  0.015591                37.0  0.290107  0.915838\n",
       "27   logistic  0.104334                19.0  0.301130  0.909321\n",
       "21       relu  0.001993                40.0  0.303154  0.908097\n",
       "29       relu  0.020123                23.0  0.307135  0.905668\n",
       "3        relu  0.160314                33.0  0.308621  0.904753\n",
       "8        tanh  0.025826                25.0  0.308811  0.904636\n",
       "6    logistic  0.005094                25.0  0.309557  0.904175\n",
       "16       tanh  0.000292                24.0  0.310537  0.903567"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather all results\n",
    "results = pd.DataFrame([\n",
    "    space_eval(search_space,row.to_dict()) for _,row in pd.DataFrame(trials.vals).iterrows()\n",
    "]) \n",
    "\n",
    "results['RMSE_CV'] = [tmp['loss'] for tmp in trials.results]\n",
    "results['R2_cv'] = 1-results['RMSE_CV']**2/y.var()\n",
    "\n",
    "results.sort_values('R2_cv',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "723da348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/surajys/opt/anaconda3/envs/msia420-2/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:536: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# fit final NN model\n",
    "nn_final = MLPRegressor(\n",
    "    hidden_layer_sizes=int(best['hidden_layer_sizes']),\n",
    "    activation = best['activation'],\n",
    "    alpha = best['alpha'],\n",
    "    # optimization settings\n",
    "    solver='lbfgs',max_iter=1000\n",
    ").fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad868b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msia420-2",
   "language": "python",
   "name": "msia420-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
